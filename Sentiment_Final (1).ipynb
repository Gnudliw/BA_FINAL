{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2524a4ca-ade1-46d5-b33b-b118869efbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import spacy\n",
    "from typing import List #Weil daraum \n",
    "import unidecode #PreProcessing\n",
    "import re #Regular Expressions PreProcessing\n",
    "import string\n",
    "from collections import Counter \n",
    "\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer #TFIDF\n",
    "from sklearn.feature_extraction.text import CountVectorizer #Document Word Matrix\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV  #Split in Trainings und Test Datensatz \n",
    "from sklearn.naive_bayes import MultinomialNB #MULTINOMIAL NAIVE BAYES CLASSIFIER\n",
    "from sklearn.naive_bayes import ComplementNB #COMPLETE NAIVE BAYES \n",
    "from sklearn.svm import SVC #SUPPORT VECTOR MACHINES\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "\n",
    "from sklearn import metrics \n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, recall_score, precision_score #BEWERTUNG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad44fc3f-cbaf-4a94-b356-38ba14609937",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup \n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "try:\n",
    "    from nltk.corpus import stopwords \n",
    "    \n",
    "except: \n",
    "    import nltk \n",
    "    nltk.downlaod(\"stopwords\")\n",
    "\n",
    "finally:\n",
    "    from nltk.corpus import stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7324a535-3736-4a1b-b272-e945f9aff667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a048fe9ec30409ab69c0393594d23c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/3.77k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "204ae5d2e4c24db29b42027348218e6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/1.85k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a74baffdbd644ef6885f2003cac1dd1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/5.10k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset sst2/default to /home/michael/.cache/huggingface/datasets/sst2/default/2.0.0/9896208a8d85db057ac50c72282bcb8fe755accc671a57dd8059d4e130961ed5...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0643c464992749e4bb138502e5a8b48b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/7.44M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/67349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1821 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sst2 downloaded and prepared to /home/michael/.cache/huggingface/datasets/sst2/default/2.0.0/9896208a8d85db057ac50c72282bcb8fe755accc671a57dd8059d4e130961ed5. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f987677e853541c6bb8920ffee0eb909",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"sst2\")\n",
    "train_data, test_data = train_test_split(dataset['train'], test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84813c26-cd0a-4f6d-b9c9-861cc9d03eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):  \n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc if not token.is_space]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "550ae118-44be-402d-b9f2-5983cb98581b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLEANING\n",
    "\n",
    "def convert_to_lowercase(text: str) -> str:\n",
    "    return text.lower()\n",
    "\n",
    "def regex(text: str) -> str:\n",
    "   \n",
    "    text = re.sub(r'\\S*@\\S*\\s?', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r\"\\'\", '', text)\n",
    "    text = re.sub(r'http[s]?://\\S+', '', text)\n",
    "    text = re.sub(r'(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'\\b\\w\\b', '', text)\n",
    "    hashtags = re.findall(r'#\\w+', text)\n",
    "    text += ' '.join(hashtags)   \n",
    "    return text\n",
    "\n",
    "def lemmatize(text: str) -> str:\n",
    "    # Lemmatisierung mit SpaCy\n",
    "    doc = nlp(text)\n",
    "    lemmatized_words = []\n",
    "    for token in doc:\n",
    "        lemmatized_words.append(token.lemma_)\n",
    "    \n",
    "    return \" \".join(lemmatized_words)\n",
    "\n",
    "\n",
    "def remove_stopwords(text: str, sw: List[str] = stopwords.words(\"english\")) -> str: \n",
    "    #Eventuell 체berarbeiten\n",
    "    additional_sw = [\"ubers\",\"uber\",\"drive\",\"gt\",\"get\",\"got\",\"go\",\"ride\",\"make\",\"would\",\"say\",\"driver\", \"nt\", \"ca\"]\n",
    "    sw = sw + additional_sw\n",
    "    text_list = text.split()\n",
    "    text_list = [word for word in text_list if word.lower() not in sw]\n",
    "    return \" \".join(text_list)\n",
    "\n",
    "def remove_punctuation(text: str, punct: str = string.punctuation) -> str:\n",
    "   \n",
    "    cleaned_text = \"\".join([char for char in text if char not in punct])\n",
    "    return cleaned_text\n",
    "\n",
    "def unicode(text: str) -> str:\n",
    "    return unidecode.unidecode(text)\n",
    "\n",
    "def clean(text: str) -> str:\n",
    "    text = unicode(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = convert_to_lowercase(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = lemmatize(text)\n",
    "    text = regex(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77be0689-8283-4fd2-9215-2b2e1187a200",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Repr채sentation BoW\n",
    "\n",
    "def bow(train_data, test_data, ngram_range=(1, 1)):\n",
    "    vectorizer = CountVectorizer(stop_words='english',ngram_range=ngram_range, tokenizer=tokenizer, preprocessor=clean)\n",
    "\n",
    "    X_train = vectorizer.fit_transform(train_data['sentence'])\n",
    "    Y_train = train_data['label']\n",
    "\n",
    "    X_test = vectorizer.transform(test_data['sentence'])\n",
    "    Y_test = test_data['label']\n",
    "\n",
    "    return X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ea7a5208-725c-4592-8a52-64968466cbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(train_data, test_data, ngram_range=(1,1)):\n",
    "    tfidf = TfidfVectorizer(ngram_range=ngram_range,preprocessor=clean)\n",
    "\n",
    "    X_train = tfidf.fit_transform(train_data['sentence'])\n",
    "    Y_train = train_data['label']\n",
    "\n",
    "    X_test = tfidf.transform(test_data['sentence'])\n",
    "    Y_test = test_data['label']\n",
    "\n",
    "    return X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "bcc758e3-47ce-473a-b0d8-14532b3199e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnb_tuning(X_train, Y_train, X_test, Y_test):\n",
    "    # Multinomialer Naive Bayes Classifier\n",
    "    MNB = MultinomialNB()\n",
    "\n",
    "    # Hyperparameter f체r Grid Search\n",
    "    parameters = {'alpha': [0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1.0]}\n",
    "\n",
    "    # Grid Search mit 5-facher Kreuzvalidierung\n",
    "    grid_search = GridSearchCV(MNB, parameters, cv=5, scoring=\"accuracy\")\n",
    "    grid_search.fit(X_train, Y_train)\n",
    "\n",
    "    best_params = grid_search.best_params_  # Beste Hyperparameter-Kombination\n",
    "    best_score = grid_search.best_score_  # Beste Bewertungsmetrik (Accuracy)\n",
    "    best_model = grid_search.best_estimator_  # Bestes Modell\n",
    "\n",
    "    # Modell auf den Trainingsdaten trainieren\n",
    "    best_model.fit(X_train, Y_train)\n",
    "\n",
    "    # Vorhersagen auf den Testdaten\n",
    "    Y_pred = best_model.predict(X_test)\n",
    "\n",
    "    # Ausgabe der Ergebnisse\n",
    "    print(\"Beste Hyperparameter: \", best_params)\n",
    "    print(\"Beste Genauigkeit: {:.2f}%\".format(best_score * 100))\n",
    "\n",
    "    # Metriken berechnen\n",
    "    accuracy = accuracy_score(Y_test, Y_pred)\n",
    "    report = classification_report(Y_test, Y_pred)\n",
    "\n",
    "    print(\"Logistische Regression - Klassifikationsbericht:\")\n",
    "    print(report)\n",
    "\n",
    "    return accuracy, report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "882d1c30-b2ba-4790-a87f-d964565cd998",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnb_tuning(X_train, Y_train, X_test, Y_test):\n",
    "    # Complete Naive Bayes Classifier\n",
    "    CNB = ComplementNB()\n",
    "\n",
    "    # Hyperparameter f체r Grid Search\n",
    "    parameters = {'alpha': [0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1.0]}\n",
    "\n",
    "    # Grid Search mit 5-facher Kreuzvalidierung\n",
    "    grid_search = GridSearchCV(CNB, parameters, cv=5, scoring=\"accuracy\")\n",
    "    grid_search.fit(X_train, Y_train)\n",
    "\n",
    "    best_params = grid_search.best_params_  # Beste Hyperparameter-Kombination\n",
    "    best_score = grid_search.best_score_  # Beste Bewertungsmetrik (Accuracy)\n",
    "    best_model = grid_search.best_estimator_  # Bestes Modell\n",
    "\n",
    "    # Modell auf den Trainingsdaten trainieren\n",
    "    best_model.fit(X_train, Y_train)\n",
    "\n",
    "    # Vorhersagen auf den Testdaten\n",
    "    Y_pred = best_model.predict(X_test)\n",
    "\n",
    "    # Ausgabe der Ergebnisse\n",
    "    print(\"Beste Hyperparameter: \", best_params)\n",
    "    print(\"Beste Genauigkeit: {:.2f}%\".format(best_score * 100))\n",
    "    print(classification_report(Y_test, Y_pred))\n",
    "\n",
    "    # Metriken berechnen\n",
    "    accuracy = accuracy_score(Y_test, Y_pred)\n",
    "    report = classification_report(Y_test, Y_pred)\n",
    "\n",
    "    print(\"CNB - Klassifikationsbericht:\")\n",
    "    print(report)\n",
    "    \n",
    "    return accuracy, report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8dd86368-bc39-4fa1-8321-f6764b91cc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svc_tuning(X_train, Y_train, X_test, Y_test, ngram_range=(1, 1)):\n",
    "    parameters = {'kernel': ['linear', 'rbf'], 'C': [0.1, 1, 10], 'gamma': ['scale', 'auto']}\n",
    "    model = GridSearchCV(SVC(), parameters, cv=5, scoring=\"accuracy\")\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    best_params = model.best_params_\n",
    "    best_score = model.best_score_\n",
    "\n",
    "    best_model = model.best_estimator_\n",
    "    Y_pred = best_model.predict(X_test)\n",
    "\n",
    "    print(\"Beste Hyperparameter: \", best_params)\n",
    "    print(\"Beste Genauigkeit: {:.2f}%\".format(best_score * 100))\n",
    "    print(classification_report(Y_test, Y_pred))\n",
    "\n",
    "    accuracy = accuracy_score(Y_test, Y_pred)\n",
    "    report = classification_report(Y_test, Y_pred)\n",
    "\n",
    "    print(\"svc - Klassifikationsbericht:\")\n",
    "    print(report)\n",
    "\n",
    "    return accuracy, report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5bb6fe1f-36ce-4db5-8a6b-e5f1e63d4c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logreg(X_train, Y_train, X_test, Y_test,max_iter=1000):\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    Y_pred = model.predict(X_test)\n",
    "    report = classification_report(Y_test, Y_pred)\n",
    "    accuracy = accuracy_score(Y_test, Y_pred)\n",
    "    print(\"LogReg\")\n",
    "    print(report)\n",
    "    return accuracy, report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2392ffca-1f70-4dc6-ab12-45ffcc02dbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnb(X_train, Y_train, X_test, Y_test):\n",
    "    model = MultinomialNB()\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    Y_pred = model.predict(X_test)\n",
    "    report = classification_report(Y_test, Y_pred)\n",
    "    accuracy = accuracy_score(Y_test, Y_pred)\n",
    "    print(\"MNB\")\n",
    "    print(report)\n",
    "    return accuracy, report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1a7c8449-32fa-4496-af1f-f9242e3511fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnb(X_train, Y_train, X_test, Y_test):\n",
    "    model = ComplementNB()\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    Y_pred = model.predict(X_test)\n",
    "    report = classification_report(Y_test, Y_pred)\n",
    "    accuracy = accuracy_score(Y_test, Y_pred)\n",
    "    print(\"CNB\")\n",
    "    print(report)\n",
    "    return accuracy, report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "bfd31b42-8119-4574-9e8d-9e921b52d676",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svc(X_train, Y_train, X_test, Y_test):\n",
    "    model = SVC()\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    Y_pred = model.predict(X_test)\n",
    "    report = classification_report(Y_test, Y_pred)\n",
    "    accuracy = accuracy_score(Y_test, Y_pred)\n",
    "    print(\"SVC\")\n",
    "    print(report)\n",
    "    return accuracy, report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d9baf72a-9898-49a1-9831-3ab2a31ed16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogReg\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.84      0.86      5909\n",
      "           1       0.88      0.91      0.90      7561\n",
      "\n",
      "    accuracy                           0.88     13470\n",
      "   macro avg       0.88      0.88      0.88     13470\n",
      "weighted avg       0.88      0.88      0.88     13470\n",
      "\n",
      "SVC\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.86      0.89      5909\n",
      "           1       0.90      0.93      0.92      7561\n",
      "\n",
      "    accuracy                           0.90     13470\n",
      "   macro avg       0.90      0.90      0.90     13470\n",
      "weighted avg       0.90      0.90      0.90     13470\n",
      "\n",
      "MNB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.83      0.84      5909\n",
      "           1       0.87      0.89      0.88      7561\n",
      "\n",
      "    accuracy                           0.86     13470\n",
      "   macro avg       0.86      0.86      0.86     13470\n",
      "weighted avg       0.86      0.86      0.86     13470\n",
      "\n",
      "CNB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.87      0.85      5909\n",
      "           1       0.89      0.86      0.87      7561\n",
      "\n",
      "    accuracy                           0.86     13470\n",
      "   macro avg       0.86      0.86      0.86     13470\n",
      "weighted avg       0.86      0.86      0.86     13470\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8614699331848552,\n",
       " '              precision    recall  f1-score   support\\n\\n           0       0.83      0.87      0.85      5909\\n           1       0.89      0.86      0.87      7561\\n\\n    accuracy                           0.86     13470\\n   macro avg       0.86      0.86      0.86     13470\\nweighted avg       0.86      0.86      0.86     13470\\n')"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#UNIGRAM Word2Vec\n",
    "X_train, Y_train, X_test, Y_test = word_to_vec(train_data, test_data, ngram_range=(1, 1))\n",
    "logreg(X_train, Y_train, X_test, Y_test)\n",
    "svc(X_train, Y_train, X_test, Y_test)\n",
    "mnb(X_train, Y_train, X_test, Y_test)\n",
    "cnb(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "551c6b4f-8b1f-4225-ad3c-3c5b7276bbbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make', 'whereaft'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogReg\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.88      0.89      5909\n",
      "           1       0.91      0.93      0.92      7561\n",
      "\n",
      "    accuracy                           0.91     13470\n",
      "   macro avg       0.91      0.90      0.91     13470\n",
      "weighted avg       0.91      0.91      0.91     13470\n",
      "\n",
      "SVC\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.86      0.89      5909\n",
      "           1       0.90      0.94      0.92      7561\n",
      "\n",
      "    accuracy                           0.91     13470\n",
      "   macro avg       0.91      0.90      0.90     13470\n",
      "weighted avg       0.91      0.91      0.91     13470\n",
      "\n",
      "MNB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.87      0.87      5909\n",
      "           1       0.90      0.90      0.90      7561\n",
      "\n",
      "    accuracy                           0.89     13470\n",
      "   macro avg       0.88      0.88      0.88     13470\n",
      "weighted avg       0.89      0.89      0.89     13470\n",
      "\n",
      "CNB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.90      0.87      5909\n",
      "           1       0.92      0.88      0.89      7561\n",
      "\n",
      "    accuracy                           0.88     13470\n",
      "   macro avg       0.88      0.89      0.88     13470\n",
      "weighted avg       0.89      0.88      0.88     13470\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8844840386043059,\n",
       " '              precision    recall  f1-score   support\\n\\n           0       0.85      0.90      0.87      5909\\n           1       0.92      0.88      0.89      7561\\n\\n    accuracy                           0.88     13470\\n   macro avg       0.88      0.89      0.88     13470\\nweighted avg       0.89      0.88      0.88     13470\\n')"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#UNIGRAM / BIGRAM  Word2Vec\n",
    "X_train, Y_train, X_test, Y_test = word_to_vec(train_data, test_data, ngram_range=(1, 2))\n",
    "logreg(X_train, Y_train, X_test, Y_test)\n",
    "svc(X_train, Y_train, X_test, Y_test)\n",
    "mnb(X_train, Y_train, X_test, Y_test)\n",
    "cnb(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "7d71e49b-64a4-4c78-a815-0bb5dd51938b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make', 'whereaft'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogReg\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.73      0.81      5909\n",
      "           1       0.82      0.95      0.88      7561\n",
      "\n",
      "    accuracy                           0.85     13470\n",
      "   macro avg       0.87      0.84      0.85     13470\n",
      "weighted avg       0.86      0.85      0.85     13470\n",
      "\n",
      "SVC\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.67      0.78      5909\n",
      "           1       0.79      0.97      0.87      7561\n",
      "\n",
      "    accuracy                           0.84     13470\n",
      "   macro avg       0.86      0.82      0.83     13470\n",
      "weighted avg       0.86      0.84      0.83     13470\n",
      "\n",
      "MNB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.74      0.80      5909\n",
      "           1       0.82      0.91      0.86      7561\n",
      "\n",
      "    accuracy                           0.84     13470\n",
      "   macro avg       0.84      0.83      0.83     13470\n",
      "weighted avg       0.84      0.84      0.84     13470\n",
      "\n",
      "CNB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.92      0.82      5909\n",
      "           1       0.92      0.74      0.82      7561\n",
      "\n",
      "    accuracy                           0.82     13470\n",
      "   macro avg       0.83      0.83      0.82     13470\n",
      "weighted avg       0.84      0.82      0.82     13470\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8198960653303637,\n",
       " '              precision    recall  f1-score   support\\n\\n           0       0.73      0.92      0.82      5909\\n           1       0.92      0.74      0.82      7561\\n\\n    accuracy                           0.82     13470\\n   macro avg       0.83      0.83      0.82     13470\\nweighted avg       0.84      0.82      0.82     13470\\n')"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#BIGRAM Word2Vec\n",
    "X_train, Y_train, X_test, Y_test = word_to_vec(train_data, test_data, ngram_range=(2, 2))\n",
    "logreg(X_train, Y_train, X_test, Y_test)\n",
    "svc(X_train, Y_train, X_test, Y_test)\n",
    "mnb(X_train, Y_train, X_test, Y_test)\n",
    "cnb(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c6e6f71a-92f1-4af9-8d98-83bdb621f408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogReg\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.85      0.86      5909\n",
      "           1       0.88      0.90      0.89      7561\n",
      "\n",
      "    accuracy                           0.88     13470\n",
      "   macro avg       0.88      0.87      0.88     13470\n",
      "weighted avg       0.88      0.88      0.88     13470\n",
      "\n",
      "SVC\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.89      0.90      5909\n",
      "           1       0.92      0.93      0.92      7561\n",
      "\n",
      "    accuracy                           0.91     13470\n",
      "   macro avg       0.91      0.91      0.91     13470\n",
      "weighted avg       0.91      0.91      0.91     13470\n",
      "\n",
      "MNB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.83      0.85      5909\n",
      "           1       0.87      0.91      0.89      7561\n",
      "\n",
      "    accuracy                           0.87     13470\n",
      "   macro avg       0.87      0.87      0.87     13470\n",
      "weighted avg       0.87      0.87      0.87     13470\n",
      "\n",
      "CNB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.87      0.86      5909\n",
      "           1       0.90      0.87      0.89      7561\n",
      "\n",
      "    accuracy                           0.87     13470\n",
      "   macro avg       0.87      0.87      0.87     13470\n",
      "weighted avg       0.87      0.87      0.87     13470\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.873645137342242,\n",
       " '              precision    recall  f1-score   support\\n\\n           0       0.84      0.87      0.86      5909\\n           1       0.90      0.87      0.89      7561\\n\\n    accuracy                           0.87     13470\\n   macro avg       0.87      0.87      0.87     13470\\nweighted avg       0.87      0.87      0.87     13470\\n')"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#UNIGRAM TFIDF\n",
    "X_train, Y_train, X_test, Y_test = tfidf(train_data, test_data, ngram_range=(1, 1))\n",
    "logreg(X_train, Y_train, X_test, Y_test)\n",
    "svc(X_train, Y_train, X_test, Y_test)\n",
    "mnb(X_train, Y_train, X_test, Y_test)\n",
    "cnb(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "fa50d8c3-13f0-4831-8ce2-a5f86c4c8eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogReg\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.87      0.89      5909\n",
      "           1       0.90      0.93      0.91      7561\n",
      "\n",
      "    accuracy                           0.90     13470\n",
      "   macro avg       0.90      0.90      0.90     13470\n",
      "weighted avg       0.90      0.90      0.90     13470\n",
      "\n",
      "SVC\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.90      0.90      5909\n",
      "           1       0.92      0.93      0.93      7561\n",
      "\n",
      "    accuracy                           0.92     13470\n",
      "   macro avg       0.91      0.91      0.91     13470\n",
      "weighted avg       0.92      0.92      0.92     13470\n",
      "\n",
      "MNB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.88      0.89      5909\n",
      "           1       0.91      0.92      0.91      7561\n",
      "\n",
      "    accuracy                           0.90     13470\n",
      "   macro avg       0.90      0.90      0.90     13470\n",
      "weighted avg       0.90      0.90      0.90     13470\n",
      "\n",
      "CNB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.91      0.89      5909\n",
      "           1       0.93      0.90      0.91      7561\n",
      "\n",
      "    accuracy                           0.90     13470\n",
      "   macro avg       0.90      0.90      0.90     13470\n",
      "weighted avg       0.90      0.90      0.90     13470\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9030438010393467,\n",
       " '              precision    recall  f1-score   support\\n\\n           0       0.88      0.91      0.89      5909\\n           1       0.93      0.90      0.91      7561\\n\\n    accuracy                           0.90     13470\\n   macro avg       0.90      0.90      0.90     13470\\nweighted avg       0.90      0.90      0.90     13470\\n')"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#UNIGRAM / BIGRAM TFIDF\n",
    "X_train, Y_train, X_test, Y_test = tfidf(train_data, test_data, ngram_range=(1, 2))\n",
    "logreg(X_train, Y_train, X_test, Y_test)\n",
    "svc(X_train, Y_train, X_test, Y_test)\n",
    "mnb(X_train, Y_train, X_test, Y_test)\n",
    "cnb(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "fd3d4797-f9bb-440b-8d4f-ab47832b4920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogReg\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.74      0.82      5909\n",
      "           1       0.82      0.95      0.88      7561\n",
      "\n",
      "    accuracy                           0.86     13470\n",
      "   macro avg       0.87      0.84      0.85     13470\n",
      "weighted avg       0.86      0.86      0.85     13470\n",
      "\n",
      "SVC\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.76      0.83      5909\n",
      "           1       0.84      0.94      0.89      7561\n",
      "\n",
      "    accuracy                           0.86     13470\n",
      "   macro avg       0.87      0.85      0.86     13470\n",
      "weighted avg       0.87      0.86      0.86     13470\n",
      "\n",
      "MNB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.76      0.82      5909\n",
      "           1       0.83      0.92      0.88      7561\n",
      "\n",
      "    accuracy                           0.85     13470\n",
      "   macro avg       0.86      0.84      0.85     13470\n",
      "weighted avg       0.86      0.85      0.85     13470\n",
      "\n",
      "CNB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.93      0.84      5909\n",
      "           1       0.93      0.77      0.85      7561\n",
      "\n",
      "    accuracy                           0.84     13470\n",
      "   macro avg       0.85      0.85      0.84     13470\n",
      "weighted avg       0.86      0.84      0.84     13470\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8415738678544915,\n",
       " '              precision    recall  f1-score   support\\n\\n           0       0.76      0.93      0.84      5909\\n           1       0.93      0.77      0.85      7561\\n\\n    accuracy                           0.84     13470\\n   macro avg       0.85      0.85      0.84     13470\\nweighted avg       0.86      0.84      0.84     13470\\n')"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#BIGRAM TFIDF\n",
    "X_train, Y_train, X_test, Y_test = tfidf(train_data, test_data, ngram_range=(2, 2))\n",
    "logreg(X_train, Y_train, X_test, Y_test)\n",
    "svc(X_train, Y_train, X_test, Y_test)\n",
    "mnb(X_train, Y_train, X_test, Y_test)\n",
    "cnb(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87ce2d7c-047f-484f-9762-b333c0b2eee4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'csr_matrix' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m X_train, Y_train, X_test, Y_test \u001b[38;5;241m=\u001b[39m word_to_vec(train_data, test_data, ngram_range\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m----> 2\u001b[0m \u001b[43msvc_tuning\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 3\u001b[0m, in \u001b[0;36msvc_tuning\u001b[0;34m(X_train, Y_train, X_test, Y_test, ngram_range)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msvc_tuning\u001b[39m(X_train, Y_train, X_test, Y_test, ngram_range\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)):\n\u001b[1;32m      2\u001b[0m     vectorizer \u001b[38;5;241m=\u001b[39m CountVectorizer(ngram_range\u001b[38;5;241m=\u001b[39mngram_range)\n\u001b[0;32m----> 3\u001b[0m     X_train \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     X_test \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mtransform(X_test)\n\u001b[1;32m      6\u001b[0m     parameters \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkernel\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrbf\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m10\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgamma\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscale\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m]}\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1383\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1375\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1376\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1377\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1378\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1379\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1380\u001b[0m             )\n\u001b[1;32m   1381\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1383\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1385\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[1;32m   1386\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1270\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1268\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[1;32m   1269\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m-> 1270\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1271\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1272\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:110\u001b[0m, in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m preprocessor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 110\u001b[0m         doc \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    112\u001b[0m         doc \u001b[38;5;241m=\u001b[39m tokenizer(doc)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:68\u001b[0m, in \u001b[0;36m_preprocess\u001b[0;34m(doc, accent_function, lower)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03mapply to a document.\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    preprocessed string\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lower:\n\u001b[0;32m---> 68\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m()\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accent_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m     doc \u001b[38;5;241m=\u001b[39m accent_function(doc)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'csr_matrix' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "X_train, Y_train, X_test, Y_test = word_to_vec(train_data, test_data, ngram_range=(1, 2))\n",
    "svc_tuning(X_train, Y_train, X_test, Y_test,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c263d97e-ad79-4e29-af42-6307ec29f6de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
